import os
import json
import uuid
from datetime import datetime
import jieba.analyse
import jieba.posseg as p_seg
import pandas

from app.utility.base_service import BaseService
from app.utility.base_service import BaseWorld

special_word_list = ["", "+", "|", ":", "ï¼š"]


class autorun_task(BaseService):
    def __init__(self, services, time_database_id, local_work_path):
        self.log = self.create_logger('autorun_task')
        self.app = services.get('app_svc')
        self.notionapi = services.get('notionapi_svc')
        self.time_database_id = time_database_id
        self.db_dir = os.path.join(local_work_path, "db")
        self.local_db_path = None
        self.Algorithm_1_db = None
        self.Algorithm_2_db = None
        self.Algorithm_3_db = None

    async def calculate_cost_time(self):
        """
        è‡ªåŠ¨è®¡ç®—æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•æ•°æ®åº“ä¸­çš„äº‹ä»¶èŠ±è´¹æ—¶é•¿
        :return:
        """
        # èŽ·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•çš„äº‹ä»¶åˆ—è¡¨
        page_size = 10
        new_pages = await self.notionapi.database_query_page(self.time_database_id, page_size=page_size + 1)
        # æŸ¥çœ‹å‰10é¡¹æ˜¯å¦æœ‰æœªå¡«èŠ±è´¹çš„æ—¶é—´çš„äº‹ä»¶ï¼Œè®¡ç®—å¹¶å¡«å…¥èŠ±è´¹çš„æ—¶é—´
        for _index in range(page_size):
            if not new_pages[_index]["properties"]["æ±‡æ€»èŠ±è´¹æ—¶é•¿"]["formula"]["number"]:
                # è®¡ç®—èŠ±è´¹æ—¶é•¿
                cost_min_time = (self.convert_ISO_8601(
                    new_pages[_index]["properties"]["è‡ªåŠ¨åˆ›å»ºæ—¥æœŸ"]["created_time"]) - self.convert_ISO_8601(
                    new_pages[_index + 1]["properties"]["è‡ªåŠ¨åˆ›å»ºæ—¥æœŸ"]["created_time"])).seconds / 60
                # å¡«å…¥èŠ±è´¹æ—¶é•¿
                properties = self.notionapi.demo_property_normal("è®¡ç®—èŠ±è´¹æ—¶é•¿(auto)", cost_min_time, "number")
                await self.notionapi.database_update_page(new_pages[_index]["id"], properties)
                self.log.info(
                    f'{new_pages[_index]["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"] + ":" + cost_min_time.__str__()}')

    @staticmethod
    def convert_ISO_8601(raw):
        return datetime.strptime(raw.split(".")[0], '%Y-%m-%dT%H:%M:%S')

    @staticmethod
    def time_event_struct(a=None, b=None, c=None, d=None, e=None, f=None, g: list = None, h: list = None):
        return {
            "äº‹ä»¶åç§°": a,
            "é¡ºä¾¿åš": b,
            "ðŸŽ°å¤§ç±»-ç»´åº¦": c,
            "ðŸ‘£å°ç±»è¡Œä¸º": d,
            "åˆ›å»ºæ—¶é—´": e,
            "æ±‡æ€»èŠ±è´¹æ—¶é•¿": f,
            "äº‹ä»¶åç§°è¯ä¹‰åˆ†æž": g,
            # "é¡ºä¾¿åšè¯ä¹‰åˆ†æž": h,
        }

    async def generate_db_path(self):
        """
        ç”Ÿæˆæœ¬å‘¨é‡‡é›†æ•°æ®çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        :return:
        """
        # åˆ¤æ–­æœ¬å‘¨æ•°æ®æ˜¯å¦åœ¨æ•°æ®åº“ä¸­
        _judge_list = [_ for _ in BaseWorld.getfile(self.db_dir) if self.local_week().split("(")[0] in _]
        if len(_judge_list) == 0:
            # æ–°å‘¨æ›´æ–°
            self.local_db_path = await self.transfo_training_set()
            await self.Algorithm_1_generate_db()
        if len(_judge_list) == 1:
            self.local_db_path = [_ for _ in BaseWorld.getfile(self.db_dir) if self.local_week().split("(")[0] in _][0]
        if len(_judge_list) > 1:
            raise Exception("[!]å¼‚å¸¸ æœ‰å¤šä¸ªåœ¨åŒå‘¨ç”Ÿæˆçš„æ•°æ®åº“æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“æ•°æ®")

    async def transfo_training_set(self):
        """
        è½¬åŒ–è®­ç»ƒé›†
        :return:
        """
        # èŽ·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•æ•°æ®åº“ä¸­çš„æ‰€æœ‰äº‹ä»¶
        time_event_db = []
        start_cursor = None
        while True:
            raw_pages = await self.notionapi.database_query_page(self.time_database_id, start_cursor=start_cursor,
                                                                 complete_resp=True)
            # æå–äº‹ä»¶åç§°ã€å¤§ç±»ã€å°ç±»ã€åˆ›å»ºæ—¶é—´ã€èŠ±è´¹æ—¶é•¿
            for page in raw_pages["results"]:
                # print(json.dumps(page["properties"], indent=4, ensure_ascii=False))
                # return
                raw_event = self.time_event_struct(
                    page["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"],
                    "" if not page["properties"]["é¡ºä¾¿åš"]["rich_text"] else page["properties"]["é¡ºä¾¿åš"]["rich_text"][0][
                        "plain_text"],
                    page["properties"]["ðŸŽ°å¤§ç±»-ç»´åº¦"]["select"],
                    page["properties"]["ðŸ‘£å°ç±»è¡Œä¸º"]["select"],
                    page["properties"]["åˆ›å»ºæ—¶é—´"]["formula"]["string"],
                    page["properties"]["æ±‡æ€»èŠ±è´¹æ—¶é•¿"]["formula"]["number"],
                    # å¯¹äº‹ä»¶åç§°ä¾æ®+åšåˆ‡å‰²ï¼Œå¹¶åˆ†æžæ¯ä¸ªæ—¶é—´çš„è¯­ä¹‰ç»„æˆ
                    self.parsing_eventName_meaning(page["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"],
                                                   single_cut=True)
                )
                # åŽ»é™¤ä¸å®Œæ•´çš„äº‹ä»¶
                if len([_ for _ in raw_event.values() if _ is None]) > 0:
                    print(raw_event.values())
                    continue
                time_event_db.append(raw_event)
            if "has_more" not in raw_pages:
                raise Exception("[!]miss has_more")
            if not raw_pages["has_more"]:
                break
            else:
                start_cursor = raw_pages["next_cursor"]
        raw_db = json.dumps(time_event_db, indent=4, ensure_ascii=False)
        db_path = os.path.join(self.db_dir, "{}_{}.json".format(self.local_week(), uuid.uuid4().__str__()))
        with open(db_path, "w", encoding="utf-8") as f:
            print(len(raw_db))
            f.write(raw_db)
        return db_path

    @staticmethod
    def local_week():
        return str(datetime.now().strftime('%Y-%W(%m-%d)'))

    async def generate_training_model(self):
        """
        ç”Ÿæˆæœºå™¨å­¦ä¹ æ¨¡åž‹
        :return:
        """
        await self.generate_db_path()
        # è¯»å–æœ¬å‘¨æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºpandasæ ¼å¼
        with open(self.local_db_path, 'r', encoding="utf-8") as f:
            raw_db = f.read()
            json_db = json.loads(raw_db)
            panda_db = pandas.json_normalize(json_db)
            print(panda_db.groupby("äº‹ä»¶åç§°").size())

    @staticmethod
    def parsing_eventName_meaning(sentence, single_cut=False, single_extract_tags=False, cut_flag=False,
                                  single_extract_flag=False):
        """
        è§£æžtitle äº‹ä»¶åç§°è¯ä¹‰
        :return:
        """
        words = p_seg.cut(sentence)
        words_list = list([_.word for _ in words if _.word.strip() not in ["", "+", "|", ":", "ï¼š"]])
        if single_cut:
            return words_list
        core_words = jieba.analyse.extract_tags(sentence, 20, allowPOS=['ns', 'n', 'vn', 'v', 'nr'], withFlag=True)
        # core_words_list = list([_.word for _ in core_words if _.word.strip() not in ["", "+", "|", ":", "ï¼š"]])
        # if single_extract_tags:
        #     return core_words_list

    async def identify_label(self):
        """
        è¯†åˆ«æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•æ•°æ®åº“ä¸­çš„äº‹ä»¶æ ‡ç­¾
        :return:
        """
        # ä½¿ç”¨æ¨¡åž‹è¯†åˆ«äº‹ä»¶çš„å¤§ç±»ã€å°ç±»

    async def update_event(self):
        """

        :return:
        """

    async def update_notion_select(self, page_id, small_OR_big, _uuid):
        """
        æ›´æ–°notioné¡µé¢çš„å¤§ç±»å°ç±»æ ‡ç­¾
        :param page_id:
        :param small_OR_big:
        :param _uuid:
        :return:
        """
        if small_OR_big == 0:
            small_OR_big = "ðŸ‘£å°ç±»è¡Œä¸º"
        elif small_OR_big == 1:
            small_OR_big = "ðŸŽ°å¤§ç±»-ç»´åº¦"
        else:
            raise Exception("[!]update_notion_select()æœªè¾“å…¥æœ‰æ•ˆçš„å¤§ç±»å°ç±»")

        # å¡«å…¥æ ‡ç­¾é€‰é¡¹
        properties = self.notionapi.demo_property_normal(small_OR_big, {"id": _uuid}, "select")
        await self.notionapi.database_update_page(page_id, properties)

    async def update_notion_autolog(self, page_id, Algorithm_name, rate):
        """
        æ›´æ–°notioné¡µé¢çš„è‡ªåŠ¨åŒ–è®°å½•
        :param page_id:
        :param Algorithm_name:
        :param rate:
        :return:
        """
        properties = self.notionapi.demo_property_text("rich_text", "è‡ªåŠ¨åŒ–è®°å½•", "{}ï¼š{}".format(Algorithm_name, rate))
        await self.notionapi.database_update_page(page_id, properties)

    async def Algorithm_1_run(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„è¿è¡Œ
        :return:
        """
        await self.generate_db_path()
        if not self.Algorithm_1_db:
            await self.Algorithm_1_generate_db()
        # èŽ·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•çš„äº‹ä»¶åˆ—è¡¨ï¼ŒèŽ·å–æœªæ ‡è®°æ ‡ç­¾çš„äº‹ä»¶
        page_size = 20
        # TODO åœ¨æŸ¥è¯¢ä¸­æ·»åŠ ç­›é€‰è¿‡æ»¤é¡¹ï¼ŒèŠ‚çº¦ç½‘ç»œèµ„æº
        new_pages = await self.notionapi.database_query_page(self.time_database_id, page_size=page_size)
        # æŸ¥çœ‹å‰10é¡¹æ˜¯å¦æœ‰æœªå¡«èŠ±è´¹çš„æ—¶é—´çš„äº‹ä»¶ï¼Œè®¡ç®—å¹¶å¡«å…¥èŠ±è´¹çš„æ—¶é—´
        for _index in range(page_size):
            # print(json.dumps(new_pages[_index], indent=4, ensure_ascii=False))
            page_name = new_pages[_index]["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"]
            # æ ¹æ®äº‹ä»¶åç§°åœ¨æ•°æ®åº“ä¸­è¿›è¡ŒåŒ¹é…
            if (not new_pages[_index]["properties"]["ðŸŽ°å¤§ç±»-ç»´åº¦"]["select"] or not new_pages[_index]["properties"]["ðŸ‘£å°ç±»è¡Œä¸º"]["select"]) \
                    and not new_pages[_index]["properties"]["è‡ªåŠ¨åŒ–è®°å½•"]["rich_text"] and page_name in self.Algorithm_1_db \
                    and self.Algorithm_1_db[page_name][1] > 0.15:
                # æ›´æ–°å‘½ä¸­çš„åŒ¹é…ç»“æžœåˆ°notionä¸­
                _uuid_list = self.Algorithm_1_db[page_name][0].split(" ")
                # å¡«å…¥æ ‡ç­¾é€‰é¡¹
                await self.update_notion_select(new_pages[_index]["id"], 1, _uuid_list[0])
                await self.update_notion_select(new_pages[_index]["id"], 0, _uuid_list[1])
                # å¡«å…¥è‡ªåŠ¨åŒ–è®°å½•
                await self.update_notion_autolog(new_pages[_index]["id"], "ç®—æ³•1", self.Algorithm_1_db[page_name][1])

    async def Algorithm_1_generate_db(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„æ¯”çŽ‡ç»Ÿè®¡
        :return:
        """
        Algorithm_1_statistics_db = {}
        with open(self.local_db_path, 'r', encoding="utf-8") as f:
            raw_db = f.read()
            json_db = json.loads(raw_db)
            for _cell in json_db:
                sum_uuid = "{} {}".format(str(_cell["ðŸŽ°å¤§ç±»-ç»´åº¦"]["id"]), str(_cell["ðŸ‘£å°ç±»è¡Œä¸º"]["id"]))
                if _cell["äº‹ä»¶åç§°"] in Algorithm_1_statistics_db:
                    if sum_uuid in Algorithm_1_statistics_db[_cell["äº‹ä»¶åç§°"]]:
                        Algorithm_1_statistics_db[_cell["äº‹ä»¶åç§°"]][sum_uuid] += 1
                    else:
                        Algorithm_1_statistics_db[_cell["äº‹ä»¶åç§°"]].update({sum_uuid: 1})
                else:
                    Algorithm_1_statistics_db.update({_cell["äº‹ä»¶åç§°"]: {sum_uuid: 1}})
            Algorithm_1_db = {}
            for _key, _value in Algorithm_1_statistics_db.items():
                all_num = 0
                _max_num = 0
                _max_uuid = None
                for _uuid, _uuid_num in _value.items():
                    all_num = all_num + _uuid_num
                    if _uuid_num > _max_num:
                        _max_num = _uuid_num
                        _max_uuid = _uuid
                Algorithm_1_db.update({_key: [_max_uuid, _max_num / all_num]})
                # TODOï¼š æŸ¥ç»™é”™æ ‡ç­¾çš„äº‹ä»¶
            self.Algorithm_1_db = Algorithm_1_db

    async def Algorithm_2_run(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„è¿è¡Œ
        :return:
        """
        await self.generate_db_path()
        if not self.Algorithm_2_db:
            await self.Algorithm_2_generate_db()
        # èŽ·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•çš„äº‹ä»¶åˆ—è¡¨ï¼ŒèŽ·å–æœªæ ‡è®°æ ‡ç­¾çš„äº‹ä»¶
        page_size = 20
        # TODO åœ¨æŸ¥è¯¢ä¸­æ·»åŠ ç­›é€‰è¿‡æ»¤é¡¹ï¼ŒèŠ‚çº¦ç½‘ç»œèµ„æº
        new_pages = await self.notionapi.database_query_page(self.time_database_id, page_size=page_size)
        # æŸ¥çœ‹å‰10é¡¹æ˜¯å¦æœ‰æœªå¡«èŠ±è´¹çš„æ—¶é—´çš„äº‹ä»¶ï¼Œè®¡ç®—å¹¶å¡«å…¥èŠ±è´¹çš„æ—¶é—´
        for _index in range(page_size):
            # print(json.dumps(new_pages[_index], indent=4, ensure_ascii=False))
            page_name = new_pages[_index]["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"]
            words = p_seg.cut(page_name)
            compare_data = []
            [compare_data.append(_.word) for _ in words if _.word.strip() not in special_word_list]
            compare_data = list(set(compare_data))
            compare_data.sort()
            compare_data = compare_data.__str__()
            # æ ¹æ®äº‹ä»¶åç§°åœ¨æ•°æ®åº“ä¸­è¿›è¡ŒåŒ¹é…
            if (not new_pages[_index]["properties"]["ðŸŽ°å¤§ç±»-ç»´åº¦"]["select"] or not new_pages[_index]["properties"]["ðŸ‘£å°ç±»è¡Œä¸º"]["select"]) \
                    and not new_pages[_index]["properties"]["è‡ªåŠ¨åŒ–è®°å½•"]["rich_text"] and compare_data in self.Algorithm_2_db \
                    and self.Algorithm_2_db[compare_data][1] > 0.3:
                # æ›´æ–°å‘½ä¸­çš„åŒ¹é…ç»“æžœåˆ°notionä¸­
                _uuid_list = self.Algorithm_2_db[compare_data][0].split(" ")
                # å¡«å…¥æ ‡ç­¾é€‰é¡¹
                await self.update_notion_select(new_pages[_index]["id"], 1, _uuid_list[0])
                await self.update_notion_select(new_pages[_index]["id"], 0, _uuid_list[1])
                # å¡«å…¥è‡ªåŠ¨åŒ–è®°å½•
                await self.update_notion_autolog(new_pages[_index]["id"], "ç®—æ³•2", self.Algorithm_2_db[compare_data][1])

    async def Algorithm_2_generate_db(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„æ¯”çŽ‡ç»Ÿè®¡
        :return:
        """
        Algorithm_2_statistics_db = {}
        with open(self.local_db_path, 'r', encoding="utf-8") as f:
            raw_db = f.read()
            json_db = json.loads(raw_db)
            for _cell in json_db:
                words = p_seg.cut(_cell["äº‹ä»¶åç§°"])
                db_cell_name = []
                [db_cell_name.append(_.word) for _ in words if _.word.strip() not in special_word_list]
                db_cell_name = list(set(db_cell_name))
                db_cell_name.sort()
                db_cell_name = db_cell_name.__str__()
                sum_uuid = "{} {}".format(str(_cell["ðŸŽ°å¤§ç±»-ç»´åº¦"]["id"]), str(_cell["ðŸ‘£å°ç±»è¡Œä¸º"]["id"]))
                if db_cell_name in Algorithm_2_statistics_db:
                    if sum_uuid in Algorithm_2_statistics_db[db_cell_name]:
                        Algorithm_2_statistics_db[db_cell_name][sum_uuid] += 1
                    else:
                        Algorithm_2_statistics_db[db_cell_name].update({sum_uuid: 1})
                else:
                    Algorithm_2_statistics_db.update({db_cell_name: {sum_uuid: 1}})
            Algorithm_2_db = {}
            for _key, _value in Algorithm_2_statistics_db.items():
                all_num = 0
                _max_num = 0
                _max_uuid = None
                for _uuid, _uuid_num in _value.items():
                    all_num = all_num + _uuid_num
                    if _uuid_num > _max_num:
                        _max_num = _uuid_num
                        _max_uuid = _uuid
                Algorithm_2_db.update({_key: [_max_uuid, _max_num / all_num]})
                # TODOï¼š æŸ¥ç»™é”™æ ‡ç­¾çš„äº‹ä»¶
            self.Algorithm_2_db = Algorithm_2_db

    async def Algorithm_3_run(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„è¿è¡Œ
        :return:
        """
        await self.generate_db_path()
        if not self.Algorithm_3_db:
            await self.Algorithm_3_generate_db()
        # èŽ·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•çš„äº‹ä»¶åˆ—è¡¨ï¼ŒèŽ·å–æœªæ ‡è®°æ ‡ç­¾çš„äº‹ä»¶
        page_size = 20
        # TODO åœ¨æŸ¥è¯¢ä¸­æ·»åŠ ç­›é€‰è¿‡æ»¤é¡¹ï¼ŒèŠ‚çº¦ç½‘ç»œèµ„æº
        new_pages = await self.notionapi.database_query_page(self.time_database_id, page_size=page_size)
        # æŸ¥çœ‹å‰10é¡¹æ˜¯å¦æœ‰æœªå¡«èŠ±è´¹çš„æ—¶é—´çš„äº‹ä»¶ï¼Œè®¡ç®—å¹¶å¡«å…¥èŠ±è´¹çš„æ—¶é—´
        for _index in range(page_size):
            # print(json.dumps(new_pages[_index], indent=4, ensure_ascii=False))
            page_name = new_pages[_index]["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"]
            compare_data = jieba.analyse.extract_tags(page_name, 20, allowPOS=['ns', 'n', 'vn', 'v', 'nr'], withFlag=False)
            compare_data = list(set(compare_data))
            compare_data.sort()
            compare_data = compare_data.__str__()
            # æ ¹æ®äº‹ä»¶åç§°åœ¨æ•°æ®åº“ä¸­è¿›è¡ŒåŒ¹é…
            if (not new_pages[_index]["properties"]["ðŸŽ°å¤§ç±»-ç»´åº¦"]["select"] or not new_pages[_index]["properties"]["ðŸ‘£å°ç±»è¡Œä¸º"]["select"]) \
                    and not new_pages[_index]["properties"]["è‡ªåŠ¨åŒ–è®°å½•"]["rich_text"] and compare_data in self.Algorithm_3_db \
                    and self.Algorithm_3_db[compare_data][1] > 0.3:
                # æ›´æ–°å‘½ä¸­çš„åŒ¹é…ç»“æžœåˆ°notionä¸­
                _uuid_list = self.Algorithm_3_db[compare_data][0].split(" ")
                # å¡«å…¥æ ‡ç­¾é€‰é¡¹
                await self.update_notion_select(new_pages[_index]["id"], 1, _uuid_list[0])
                await self.update_notion_select(new_pages[_index]["id"], 0, _uuid_list[1])
                # å¡«å…¥è‡ªåŠ¨åŒ–è®°å½•
                await self.update_notion_autolog(new_pages[_index]["id"], "ç®—æ³•3", self.Algorithm_3_db[compare_data][1])

    async def Algorithm_3_generate_db(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„æ¯”çŽ‡ç»Ÿè®¡
        :return:
        """
        Algorithm_3_statistics_db = {}
        with open(self.local_db_path, 'r', encoding="utf-8") as f:
            raw_db = f.read()
            json_db = json.loads(raw_db)
            for _cell in json_db:
                db_cell_name = jieba.analyse.extract_tags(_cell["äº‹ä»¶åç§°"], 20, allowPOS=['ns', 'n', 'vn', 'v', 'nr'], withFlag=False)
                db_cell_name = list(set(db_cell_name))
                db_cell_name.sort()
                db_cell_name = db_cell_name.__str__()
                sum_uuid = "{} {}".format(str(_cell["ðŸŽ°å¤§ç±»-ç»´åº¦"]["id"]), str(_cell["ðŸ‘£å°ç±»è¡Œä¸º"]["id"]))
                if db_cell_name in Algorithm_3_statistics_db:
                    if sum_uuid in Algorithm_3_statistics_db[db_cell_name]:
                        Algorithm_3_statistics_db[db_cell_name][sum_uuid] += 1
                    else:
                        Algorithm_3_statistics_db[db_cell_name].update({sum_uuid: 1})
                else:
                    Algorithm_3_statistics_db.update({db_cell_name: {sum_uuid: 1}})
            Algorithm_3_db = {}
            for _key, _value in Algorithm_3_statistics_db.items():
                all_num = 0
                _max_num = 0
                _max_uuid = None
                for _uuid, _uuid_num in _value.items():
                    all_num = all_num + _uuid_num
                    if _uuid_num > _max_num:
                        _max_num = _uuid_num
                        _max_uuid = _uuid
                Algorithm_3_db.update({_key: [_max_uuid, _max_num / all_num]})
                # TODOï¼š æŸ¥ç»™é”™æ ‡ç­¾çš„äº‹ä»¶
            self.Algorithm_3_db = Algorithm_3_db

    async def Algorithm_generate_db(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„æ¯”çŽ‡ç»Ÿè®¡
        :return:
        """
        Algorithm_3_statistics_db = {}
        with open(self.local_db_path, 'r', encoding="utf-8") as f:
            raw_db = f.read()
            json_db = json.loads(raw_db)
            for _cell in json_db:
                db_cell_name = jieba.analyse.extract_tags(_cell["äº‹ä»¶åç§°"], 20, allowPOS=['ns', 'n', 'vn', 'v', 'nr'], withFlag=False)
                db_cell_name = list(set(db_cell_name)).sort()
                sum_uuid = "{} {}".format(str(_cell["ðŸŽ°å¤§ç±»-ç»´åº¦"]["id"]), str(_cell["ðŸ‘£å°ç±»è¡Œä¸º"]["id"]))
                if db_cell_name in Algorithm_3_statistics_db:
                    if sum_uuid in Algorithm_3_statistics_db[db_cell_name]:
                        Algorithm_3_statistics_db[db_cell_name][sum_uuid] += 1
                    else:
                        Algorithm_3_statistics_db[db_cell_name].update({sum_uuid: 1})
                else:
                    Algorithm_3_statistics_db.update({db_cell_name: {sum_uuid: 1}})
            Algorithm_3_db = {}
            for _key, _value in Algorithm_3_statistics_db.items():
                all_num = 0
                _max_num = 0
                _max_uuid = None
                for _uuid, _uuid_num in _value.items():
                    all_num = all_num + _uuid_num
                    if _uuid_num > _max_num:
                        _max_num = _uuid_num
                        _max_uuid = _uuid
                Algorithm_3_db.update({_key: [_max_uuid, _max_num / all_num]})
                # TODOï¼š æŸ¥ç»™é”™æ ‡ç­¾çš„äº‹ä»¶
            self.Algorithm_3_db = Algorithm_3_db

    async def run(self):
        scheduler = self.app.get_scheduler()
        scheduler.add_job(self.calculate_cost_time, 'interval', seconds=600)
        # scheduler.add_job(self.transfo_training_set, 'cron', day_of_week=1, hour=11)
        # await self.transfo_training_set()
        # await self.generate_training_model()
        # await self.Algorithm_1_run()
        await self.Algorithm_2_run()


if __name__ == '__main__':
    pass
