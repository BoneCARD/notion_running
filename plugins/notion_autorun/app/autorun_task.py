import os
import json
import uuid
from datetime import datetime
import jieba.analyse
import jieba.posseg as p_seg
import pandas

from app.utility.base_service import BaseService
from app.utility.base_service import BaseWorld

special_word_list = ["", "+", "|", ":", "ï¼š"]


class autorun_task(BaseService):
    def __init__(self, services, time_database_id, local_work_path):
        self.log = self.create_logger('autorun_task')
        self.app = services.get('app_svc')
        self.notionapi = services.get('notionapi_svc')
        self.time_database_id = time_database_id
        self.db_dir = os.path.join(local_work_path, "db")
        self.local_db_path = None
        self.N_Algorithm_info = [
            {
                "name": "ç®—æ³•1",
                "db": None,
                "generate": lambda _name: _name,
                "rate": 0.15
            },
            {
                "name": "ç®—æ³•2",
                "db": None,
                "generate": lambda _name: self._sort(self._cut(_name)),
                "rate": 0.3
            },
            {
                "name": "ç®—æ³•3",
                "db": None,
                "generate": lambda _name: self._sort(jieba.analyse.extract_tags(_name, 20, allowPOS=['ns', 'n', 'vn', 'v', 'nr'], withFlag=False)),
                "rate": 0.3
            },
        ]

    async def calculate_cost_time(self):
        """
        è‡ªåŠ¨è®¡ç®—æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•æ•°æ®åº“ä¸­çš„äº‹ä»¶èŠ±è´¹æ—¶é•¿
        :return:
        """
        # è·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•çš„äº‹ä»¶åˆ—è¡¨
        page_size = 10
        new_pages = await self.notionapi.database_query_page(self.time_database_id, page_size=page_size + 1)
        # æŸ¥çœ‹å‰10é¡¹æ˜¯å¦æœ‰æœªå¡«èŠ±è´¹çš„æ—¶é—´çš„äº‹ä»¶ï¼Œè®¡ç®—å¹¶å¡«å…¥èŠ±è´¹çš„æ—¶é—´
        for _index in range(page_size):
            if not new_pages[_index]["properties"]["æ±‡æ€»èŠ±è´¹æ—¶é•¿"]["formula"]["number"]:
                # è®¡ç®—èŠ±è´¹æ—¶é•¿
                cost_min_time = (self.convert_ISO_8601(
                    new_pages[_index]["properties"]["è‡ªåŠ¨åˆ›å»ºæ—¥æœŸ"]["created_time"]) - self.convert_ISO_8601(
                    new_pages[_index + 1]["properties"]["è‡ªåŠ¨åˆ›å»ºæ—¥æœŸ"]["created_time"])).seconds / 60
                # å¡«å…¥èŠ±è´¹æ—¶é•¿
                properties = self.notionapi.demo_property_normal("è®¡ç®—èŠ±è´¹æ—¶é•¿(auto)", cost_min_time, "number")
                await self.notionapi.database_update_page(new_pages[_index]["id"], properties)
                self.log.info(
                    f'{new_pages[_index]["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"] + ":" + cost_min_time.__str__()}')

    @staticmethod
    def convert_ISO_8601(raw):
        return datetime.strptime(raw.split(".")[0], '%Y-%m-%dT%H:%M:%S')

    @staticmethod
    def time_event_struct(a=None, b=None, c=None, d=None, e=None, f=None):
        return {
            "äº‹ä»¶åç§°": a,
            "é¡ºä¾¿åš": b,
            "ğŸ°å¤§ç±»-ç»´åº¦": c,
            "ğŸ‘£å°ç±»è¡Œä¸º": d,
            "åˆ›å»ºæ—¶é—´": e,
            "æ±‡æ€»èŠ±è´¹æ—¶é•¿": f
        }

    async def generate_db_path(self):
        """
        ç”Ÿæˆæœ¬å‘¨é‡‡é›†æ•°æ®çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        :return:
        """
        # åˆ¤æ–­æœ¬å‘¨æ•°æ®æ˜¯å¦åœ¨æ•°æ®åº“ä¸­
        _judge_list = [_ for _ in BaseWorld.getfile(self.db_dir) if self.local_week().split("(")[0] in _]
        if len(_judge_list) == 0:
            # æ–°å‘¨æ›´æ–°
            self.local_db_path = await self.transfo_training_set()
            # await self.Algorithm_1_generate_db()
            for _ in self.N_Algorithm_info:
                _["db"] = await self.Algorithm_generate_db1(_["generate"])
        if len(_judge_list) == 1:
            self.local_db_path = [_ for _ in BaseWorld.getfile(self.db_dir) if self.local_week().split("(")[0] in _][0]
        if len(_judge_list) > 1:
            raise Exception("[!]å¼‚å¸¸ æœ‰å¤šä¸ªåœ¨åŒå‘¨ç”Ÿæˆçš„æ•°æ®åº“æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“æ•°æ®")

    async def transfo_training_set(self):
        """
        è½¬åŒ–è®­ç»ƒé›†
        :return:
        """
        # è·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•æ•°æ®åº“ä¸­çš„æ‰€æœ‰äº‹ä»¶
        time_event_db = []
        start_cursor = None
        while True:
            raw_pages = await self.notionapi.database_query_page(self.time_database_id, start_cursor=start_cursor,
                                                                 complete_resp=True)
            # æå–äº‹ä»¶åç§°ã€å¤§ç±»ã€å°ç±»ã€åˆ›å»ºæ—¶é—´ã€èŠ±è´¹æ—¶é•¿
            for page in raw_pages["results"]:
                raw_event = self.time_event_struct(
                    page["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"],
                    "" if not page["properties"]["é¡ºä¾¿åš"]["rich_text"] else page["properties"]["é¡ºä¾¿åš"]["rich_text"][0][
                        "plain_text"],
                    page["properties"]["ğŸ°å¤§ç±»-ç»´åº¦"]["select"],
                    page["properties"]["ğŸ‘£å°ç±»è¡Œä¸º"]["select"],
                    page["properties"]["åˆ›å»ºæ—¶é—´"]["formula"]["string"],
                    page["properties"]["æ±‡æ€»èŠ±è´¹æ—¶é•¿"]["formula"]["number"],
                )
                # å»é™¤ä¸å®Œæ•´çš„äº‹ä»¶
                if len([_ for _ in raw_event.values() if _ is None]) > 0:
                    # print(raw_event.values())
                    continue
                time_event_db.append(raw_event)
            if "has_more" not in raw_pages:
                raise Exception("[!]miss has_more")
            if not raw_pages["has_more"]:
                break
            else:
                start_cursor = raw_pages["next_cursor"]
        raw_db = json.dumps(time_event_db, indent=4, ensure_ascii=False)
        db_path = os.path.join(self.db_dir, "{}_{}.json".format(self.local_week(), uuid.uuid4().__str__()))
        with open(db_path, "w", encoding="utf-8") as f:
            print(len(raw_db))
            f.write(raw_db)
        return db_path

    @staticmethod
    def local_week():
        return str(datetime.now().strftime('%Y-%W(%m-%d)'))

    # async def generate_training_model(self):
    #     """
    #     ç”Ÿæˆæœºå™¨å­¦ä¹ æ¨¡å‹
    #     :return:
    #     """
    #     await self.generate_db_path()
    #     # è¯»å–æœ¬å‘¨æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºpandasæ ¼å¼
    #     with open(self.local_db_path, 'r', encoding="utf-8") as f:
    #         raw_db = f.read()
    #         json_db = json.loads(raw_db)
    #         panda_db = pandas.json_normalize(json_db)
    #         print(panda_db.groupby("äº‹ä»¶åç§°").size())

    # @staticmethod
    # def parsing_eventName_meaning(sentence, single_cut=False, single_extract_tags=False, cut_flag=False,
    #                               single_extract_flag=False):
    #     """
    #     è§£ætitle äº‹ä»¶åç§°è¯ä¹‰
    #     :return:
    #     """
    #     words = p_seg.cut(sentence)
    #     words_list = list([_.word for _ in words if _.word.strip() not in ["", "+", "|", ":", "ï¼š"]])
    #     if single_cut:
    #         return words_list
    #     core_words = jieba.analyse.extract_tags(sentence, 20, allowPOS=['ns', 'n', 'vn', 'v', 'nr'], withFlag=True)
    #     # core_words_list = list([_.word for _ in core_words if _.word.strip() not in ["", "+", "|", ":", "ï¼š"]])
    #     # if single_extract_tags:
    #     #     return core_words_list

    async def update_notion_select(self, page_id, small_OR_big, _uuid):
        """
        æ›´æ–°notioné¡µé¢çš„å¤§ç±»å°ç±»æ ‡ç­¾
        :param page_id:
        :param small_OR_big:
        :param _uuid:
        :return:
        """
        if small_OR_big == 0:
            small_OR_big = "ğŸ‘£å°ç±»è¡Œä¸º"
        elif small_OR_big == 1:
            small_OR_big = "ğŸ°å¤§ç±»-ç»´åº¦"
        else:
            raise Exception("[!]update_notion_select()æœªè¾“å…¥æœ‰æ•ˆçš„å¤§ç±»å°ç±»")

        # å¡«å…¥æ ‡ç­¾é€‰é¡¹
        properties = self.notionapi.demo_property_normal(small_OR_big, {"id": _uuid}, "select")
        await self.notionapi.database_update_page(page_id, properties)

    async def update_notion_autolog(self, page_id, Algorithm_name, rate):
        """
        æ›´æ–°notioné¡µé¢çš„è‡ªåŠ¨åŒ–è®°å½•
        :param page_id:
        :param Algorithm_name:
        :param rate:
        :return:
        """
        properties = self.notionapi.demo_property_text("rich_text", "è‡ªåŠ¨åŒ–è®°å½•", "{}ï¼š{}".format(Algorithm_name, rate))
        await self.notionapi.database_update_page(page_id, properties)

    async def Algorithm_generate_db1(self, _generate):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„æ¯”ç‡ç»Ÿè®¡
        :return:
        """
        Algorithm_statistics_db = {}
        with open(self.local_db_path, 'r', encoding="utf-8") as f:
            raw_db = f.read()
            json_db = json.loads(raw_db)
            for _cell in json_db:
                db_cell_name = _generate(_cell["äº‹ä»¶åç§°"])
                # æ•°æ®åº“ä¸­ä»¥å¤§ç±»ã€å°ç±»uuidçš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œè¿™ä¹Ÿæ˜¯db1æ•°æ®åº“çš„åŒ¹é…æ ‡è¯†
                sum_uuid = "{} {}".format(str(_cell["ğŸ°å¤§ç±»-ç»´åº¦"]["id"]), str(_cell["ğŸ‘£å°ç±»è¡Œä¸º"]["id"]))
                # _liståˆ—è¡¨ç”¨äºä¸‹æ–¹forå¾ªç¯ï¼ŒåŠ å¾ªç¯çš„ç›®çš„æ˜¯ä¸ºäº†å¯¹æœ‰å†’å·:ï¼šçš„äº‹ä»¶åç§°åšåˆ†å‰²æå–ï¼Œæå–å†’å·å‰é¢çš„å­—æ®µåŠ å…¥çš„æ•°æ®åº“ä¸­åšç»Ÿè®¡
                _list = [db_cell_name]
                [_list.append(db_cell_name.split(_)[0]) for _ in [_ for _ in [":", "ï¼š"] if _ in db_cell_name]]
                for range_index in range(2 if len(_list) >= 2 else 1):
                    # åˆ¤æ–­å­—æ®µæ˜¯å¦åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœåœ¨å­—æ®µå€¼åŠ ä¸€ï¼Œä¸åœ¨åˆ™æ–°å¢è¯¥å­—æ®µåˆ°æ•°æ®åº“ä¸­
                    if _list[range_index] in Algorithm_statistics_db:
                        if sum_uuid in Algorithm_statistics_db[_list[range_index]]:
                            Algorithm_statistics_db[_list[range_index]][sum_uuid] += 1
                        else:
                            Algorithm_statistics_db[_list[range_index]].update({sum_uuid: 1})
                    else:
                        Algorithm_statistics_db.update({_list[range_index]: {sum_uuid: 1}})

            Algorithm_db = {}
            for _key, _value in Algorithm_statistics_db.items():
                # ç»Ÿè®¡æ‰€æœ‰å•å…ƒçš„æ•°é‡æ€»å’Œï¼ˆå•å…ƒå°±æ˜¯ä¸Šé¢çš„db_cell_nameï¼‰
                all_num = 0
                # è®°å½•å•å…ƒä¸­æœ€é«˜çš„æ•°é‡ï¼ˆæ¯ä¸ªå•å…ƒéƒ½æœ‰è‡ªå·±æ•°é‡ï¼‰
                _max_num = 0
                # è®°å½•æœ€å¤§æ•°é‡çš„å•å…ƒuuid
                _max_uuid = None
                for _uuid, _uuid_num in _value.items():
                    all_num = all_num + _uuid_num
                    if _uuid_num > _max_num:
                        _max_num = _uuid_num
                        _max_uuid = _uuid
                Algorithm_db.update({_key: [_max_uuid, _max_num / all_num]})
            # print(json.dumps(Algorithm_db, indent=4, ensure_ascii=False))
            return Algorithm_db

    # async def Algorithm_generate_db2(self, _generate):
    #     """
    #
    #     :param _generate:
    #     :return:
    #     """
    #     Algorithm_statistics_db = {}
    #     with open(self.local_db_path, 'r', encoding="utf-8") as f:
    #         raw_db = f.read()
    #         json_db = json.loads(raw_db)
    #         for _cell in json_db:
    #             db_cell_name_list = _generate(_cell["äº‹ä»¶åç§°"])
    #             sum_uuid = "{} {}".format(str(_cell["ğŸ°å¤§ç±»-ç»´åº¦"]["id"]), str(_cell["ğŸ‘£å°ç±»è¡Œä¸º"]["id"]))
    #             _list = [db_cell_name]
    #             [_list.append(db_cell_name.split(_)[0]) for _ in [_ for _ in [":", "ï¼š"] if _ in db_cell_name]]
    #             for range_index in range(2 if len(_list) >= 2 else 1):
    #                 if _list[range_index] in Algorithm_statistics_db:
    #                     if sum_uuid in Algorithm_statistics_db[_list[range_index]]:
    #                         Algorithm_statistics_db[_list[range_index]][sum_uuid] += 1
    #                     else:
    #                         Algorithm_statistics_db[_list[range_index]].update({sum_uuid: 1})
    #                 else:
    #                     Algorithm_statistics_db.update({_list[range_index]: {sum_uuid: 1}})
    #
    #         Algorithm_db = {}
    #         for _key, _value in Algorithm_statistics_db.items():
    #             all_num = 0
    #             _max_num = 0
    #             _max_uuid = None
    #             for _uuid, _uuid_num in _value.items():
    #                 all_num = all_num + _uuid_num
    #                 if _uuid_num > _max_num:
    #                     _max_num = _uuid_num
    #                     _max_uuid = _uuid
    #             Algorithm_db.update({_key: [_max_uuid, _max_num / all_num]})
    #         # print(json.dumps(Algorithm_db, indent=4, ensure_ascii=False))
    #         return Algorithm_db

    async def Algorithm_run(self):
        """
        å®Œå…¨åŒ¹é…çš„åœºæ™¯:ç»Ÿè®¡â€œäº‹ä»¶åç§°â€çš„è¿è¡Œ
        :return:
        """
        await self.generate_db_path()
        for _ in self.N_Algorithm_info:
            if not _["db"]:
                _["db"] = await self.Algorithm_generate_db1(_["generate"])
        # è·å–æŸ³æ¯”æ­‡å¤«æ—¶é—´ç»Ÿè®¡æ³•çš„äº‹ä»¶åˆ—è¡¨ï¼Œè·å–æœªæ ‡è®°æ ‡ç­¾çš„äº‹ä»¶
        page_size = 20
        _filter = {
            "and": [
                {
                    "or": [
                        {
                            "property": "ğŸ°å¤§ç±»-ç»´åº¦",
                            "select": {
                                "is_empty": True
                            }
                        },
                        {
                            "property": "ğŸ‘£å°ç±»è¡Œä¸º",
                            "select": {
                                "is_empty": True
                            }
                        },
                    ],
                },
                {
                    "property": "è‡ªåŠ¨åŒ–è®°å½•",
                    "rich_text": {
                        "is_empty": True
                    }
                },
            ]
        }
        new_pages = await self.notionapi.database_query_page(self.time_database_id, _filter=_filter, page_size=page_size)
        # æŸ¥çœ‹å‰10é¡¹æ˜¯å¦æœ‰æœªå¡«èŠ±è´¹çš„æ—¶é—´çš„äº‹ä»¶ï¼Œè®¡ç®—å¹¶å¡«å…¥èŠ±è´¹çš„æ—¶é—´
        for _index in range(len(new_pages)):
            N_flag = False
            for _ in self.N_Algorithm_info:
                page_name = new_pages[_index]["properties"]["äº‹ä»¶åç§°"]["title"][0]["plain_text"]
                compare_data = _["generate"](page_name)
                # æ ¹æ®äº‹ä»¶åç§°åœ¨æ•°æ®åº“ä¸­è¿›è¡ŒåŒ¹é…
                if compare_data in _["db"] and _["db"][compare_data][1] > _["rate"]:
                    # æ›´æ–°å‘½ä¸­çš„åŒ¹é…ç»“æœåˆ°notionä¸­
                    _uuid_list = _["db"][compare_data][0].split(" ")
                    # å¡«å…¥æ ‡ç­¾é€‰é¡¹
                    await self.update_notion_select(new_pages[_index]["id"], 1, _uuid_list[0])
                    await self.update_notion_select(new_pages[_index]["id"], 0, _uuid_list[1])
                    # å¡«å…¥è‡ªåŠ¨åŒ–è®°å½•
                    await self.update_notion_autolog(new_pages[_index]["id"], _["name"], _["db"][compare_data][1])
                    N_flag = True
                    break

    @staticmethod
    def _sort(_list):
        list(set(_list)).sort()
        return _list.__str__()

    @staticmethod
    def _cut(sentence, withFlag=False):
        if not withFlag:
            _data = []
            [_data.append(_.word) for _ in p_seg.cut(sentence) if _.word.strip() not in special_word_list]
        else:
            _data = {}
            [_data.update({_.word: _.flag}) for _ in p_seg.cut(sentence) if _.word.strip() not in special_word_list]
        return _data

    async def run(self):
        scheduler = self.app.get_scheduler()
        scheduler.add_job(self.calculate_cost_time, 'interval', seconds=600)
        scheduler.add_job(self.Algorithm_run, 'interval', seconds=600)
        scheduler.add_job(self.transfo_training_set, 'cron', day_of_week=1, hour=11)
        # await self.transfo_training_set()
        # await self.Algorithm_run()


if __name__ == '__main__':
    pass
